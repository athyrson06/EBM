{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils import Bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = [\"issue_d\", \"loan_amnt\", \"funded_amnt\", \"funded_amnt_inv\", \"term\", \"int_rate\"]\n",
    "lb = [\"installment\", \"emp_length\", \"annual_inc\", \"verification_status\", \"loan_status\", \n",
    "      \"purpose\", \"addr_state\", \"dti\", \"delinq_2yrs\"]\n",
    "lc = [\"inq_last_6mths\", \"open_acc\", \"home_ownership\", \"revol_bal\", \"revol_util\",\n",
    "       \"total_acc\", \"total_pymnt\", \"total_rec_prncp\", \"total_rec_int\", \"total_pymnt_inv\",\n",
    "         \"last_pymnt_amnt\", \"last_fico_range_high\", \"last_fico_range_low\"]\n",
    "\n",
    "selected_columns_a = la+lb+lc\n",
    "\n",
    "# Define the chunk size for reading the CSV file\n",
    "chunksize = 100000  # Adjust this value based on your requirements\n",
    "\n",
    "# Initialize an empty list to store filtered chunks\n",
    "filtered_chunks = []\n",
    "\n",
    "# Read the CSV file in chunks based on the defined chunk size\n",
    "for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize, usecols=selected_columns_a):\n",
    "    # Filter the current chunk based on the criteria\n",
    "    filtered_chunk = chunk[chunk['issue_d'].str.contains(\"2009\", na=False)]\n",
    "    # filtered_chunk = filtered_chunk[~filtered_chunk['issue_d'].str.contains(\"Oct-2013|Nov-2013|Dec-2013\", na=False)]\n",
    "    # Append the filtered chunk to the list\n",
    "    filtered_chunks.append(filtered_chunk)\n",
    "\n",
    "# Concatenate all filtered chunks into a single DataFrame\n",
    "df = pd.concat(filtered_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df.sample(frac=0.05, random_state=42)\n",
    "df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Calculate risk score\n",
    "    df[\"risk_score\"] = df.loc[:, [\"last_fico_range_high\", \"last_fico_range_low\"]].mean(axis=1)\n",
    "    \n",
    "    # Create target variable\n",
    "    df[\"target\"] = np.where((df.loan_status == 'Current') |\n",
    "                            (df.loan_status == 'Fully Paid') |\n",
    "                            (df.loan_status == \"Issued\") |\n",
    "                            (df.loan_status == 'Does not meet the credit policy. Status:Fully Paid'), 0, 1)\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    for c in ['last_fico_range_high', 'last_fico_range_low', 'loan_status']:\n",
    "        df = df.drop(c, axis=1, errors='ignore')\n",
    "    \n",
    "    # Columns based on Shih et al. (2022)\n",
    "    pearson_a = ['int_rate', 'dti', 'delinq_2yrs', 'emp_length', 'annual_inc', 'inq_last_6mths', 'term',\n",
    "                 'home_ownership', 'revol_util', 'risk_score', 'issue_d', 'target']\n",
    "    \n",
    "    # Select columns\n",
    "    df = df.loc[:, pearson_a]\n",
    "    \n",
    "    \n",
    "    def fix_term_column(df):\n",
    "        \"\"\"\n",
    "        Fix the dtype of the 'term' column by extracting the numeric part and converting it to an integer.\n",
    "        Handles cases where the column contains non-string values (e.g., NaN or float).\n",
    "        \n",
    "        Parameters:\n",
    "            df (pd.DataFrame): The DataFrame containing the 'term' column.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame with the 'term' column converted to numeric.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract numeric part from the 'term' column and convert to integer\n",
    "            df['term'] = df['term'].apply(lambda x: int(re.search(r'\\d+', str(x)).group()) if pd.notna(x) else x)\n",
    "        except AttributeError as e:\n",
    "            # Handle cases where the regex search fails (e.g., no match found)\n",
    "            print(\"Error: Unable to extract numeric value from 'term' column.\")\n",
    "            print(f\"Details: {e}\")\n",
    "        except Exception as e:\n",
    "            # Handle any other unexpected errors\n",
    "            print(\"An unexpected error occurred while fixing the 'term' column.\")\n",
    "            print(f\"Details: {e}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def fix_emp_length_column(df):\n",
    "        \"\"\"\n",
    "        Fix the dtype of the 'emp_length' column by converting it to numeric.\n",
    "        Handles cases where the column contains non-string values (e.g., NaN or float).\n",
    "        \n",
    "        Parameters:\n",
    "            df (pd.DataFrame): The DataFrame containing the 'emp_length' column.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame with the 'emp_length' column converted to numeric.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Map '< 1 year' to '0' and extract numeric part from the 'emp_length' column\n",
    "            df['emp_length'] = df['emp_length'].map(lambda x: \"0\" if x == '< 1 year' else x)\n",
    "            df['emp_length'] = df['emp_length'].map(lambda x: int(re.search(r'\\d+', str(x)).group()), na_action='ignore')\n",
    "        except Exception as e:\n",
    "            # Handle any unexpected errors\n",
    "            print(\"Error: Unable to convert 'emp_length' column to numeric.\")\n",
    "            print(f\"Details: {e}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    df = fix_term_column(df)\n",
    "    df = fix_emp_length_column(df)\n",
    "\n",
    "    df = df.dropna(axis=0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = preprocess_data(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_df = df_2.loc[:, ['risk_score', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Histogram for target = 0\n",
    "ax1.hist(risk_df[risk_df['target'] == 0]['risk_score'], bins=60, edgecolor='black', alpha=0.7, color='blue', label='Target 0')\n",
    "ax1.set_xlabel('Risk Score')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Histogram of Risk Scores (Target = 0)')\n",
    "ax1.legend()\n",
    "\n",
    "# Histogram for target = 1\n",
    "ax2.hist(risk_df[risk_df['target'] == 1]['risk_score'], bins=60, edgecolor='black', alpha=0.7, color='red', label='Target 1')\n",
    "ax2.set_xlabel('Risk Score')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Histogram of Risk Scores (Target = 1)')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to avoid overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, balanced_accuracy_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Function to evaluate classifiers\n",
    "def evaluate_classifiers(X_train, X_test, y_train, y_test):\n",
    "    classifiers = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.01, max_depth=3, random_state=42)\n",
    "    }\n",
    "    \n",
    "    metrics = {\n",
    "        'AUC': roc_auc_score,\n",
    "        'Balanced Accuracy': balanced_accuracy_score,\n",
    "        'Recall': recall_score\n",
    "    }\n",
    "    \n",
    "    results = {name: {} for name in classifiers.keys()}\n",
    "    \n",
    "    for clf_name, clf in classifiers.items():\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        for metric_name, metric_func in metrics.items():\n",
    "            if metric_name == 'AUC':\n",
    "                score = metric_func(y_test, y_pred_proba)\n",
    "            else:\n",
    "                score = metric_func(y_test, y_pred)\n",
    "            results[clf_name][metric_name] = score\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Prepare datasets\n",
    "risk_df_zero_min = risk_df.copy()\n",
    "risk_df_zero_min['risk_score'] = risk_df_zero_min['risk_score'] - risk_df_zero_min['risk_score'].min()\n",
    "\n",
    "# Filter the second dataset to include only samples where risk_score > 500\n",
    "risk_df_500_min = risk_df[risk_df['risk_score'] > 500].copy()\n",
    "\n",
    "# Split datasets\n",
    "X_zero_min = risk_df_zero_min[['risk_score']]\n",
    "y_zero_min = risk_df_zero_min['target']\n",
    "X_train_zero_min, X_test_zero_min, y_train_zero_min, y_test_zero_min = train_test_split(X_zero_min, y_zero_min, test_size=0.2, random_state=42)\n",
    "\n",
    "X_500_min = risk_df_500_min[['risk_score']]\n",
    "y_500_min = risk_df_500_min['target']\n",
    "X_train_500_min, X_test_500_min, y_train_500_min, y_test_500_min = train_test_split(X_500_min, y_500_min, test_size=0.2, random_state=42)\n",
    "\n",
    "# Evaluate classifiers\n",
    "results_zero_min = evaluate_classifiers(X_train_zero_min, X_test_zero_min, y_train_zero_min, y_test_zero_min)\n",
    "results_500_min = evaluate_classifiers(X_train_500_min, X_test_500_min, y_train_500_min, y_test_500_min)\n",
    "\n",
    "print(\"Evaluation Metrics with Risk Score Min = 0:\")\n",
    "for clf_name, metrics in results_zero_min.items():\n",
    "    print(f\"{clf_name}:\")\n",
    "    for metric_name, score in metrics.items():\n",
    "        print(f\"  {metric_name}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluation Metrics with Risk Score > 500:\")\n",
    "for clf_name, metrics in results_500_min.items():\n",
    "    print(f\"{clf_name}:\")\n",
    "    for metric_name, score in metrics.items():\n",
    "        print(f\"  {metric_name}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_df_500_min.mean(), risk_df_zero_min.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Histogram for target = 0\n",
    "ax1.hist(risk_df_500_min[risk_df_500_min['target'] == 0]['risk_score'], bins=60, edgecolor='black', alpha=0.7, color='blue', label='Target 0')\n",
    "ax1.set_xlabel('Risk Score')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Histogram of Risk Scores (Target = 0)')\n",
    "ax1.legend()\n",
    "\n",
    "# Histogram for target = 1\n",
    "ax2.hist(risk_df_500_min[risk_df_500_min['target'] == 1]['risk_score'], bins=60, edgecolor='black', alpha=0.7, color='red', label='Target 1')\n",
    "ax2.set_xlabel('Risk Score')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Histogram of Risk Scores (Target = 1)')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to avoid overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_2.loc[:, df_2.columns != 'target'].drop(['issue_d', 'home_ownership', 'risk_score'], axis=1)\n",
    "y = df_2['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bunch_df(X,y):\n",
    "    # Convert to NumPy arrays\n",
    "    X_array = X.to_numpy()\n",
    "    y_array = y.to_numpy()\n",
    "\n",
    "    # Get feature names\n",
    "    feature_names = X.columns.tolist()\n",
    "\n",
    "    # Create a Bunch object\n",
    "    bunch_df = Bunch(\n",
    "        data=X_array,\n",
    "        target=y_array,\n",
    "        feature_names=feature_names,\n",
    "        DESCR=\"Custom dataset similar to scikit-learn fetch_datasets\"\n",
    "    )\n",
    "    return bunch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = create_bunch_df(X, y)\n",
    "X = df_3.data\n",
    "y = df_3.target\n",
    "feature_names = df_3.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test, and feature_names are already defined\n",
    "n_features = X_train.shape[1]\n",
    "models = []\n",
    "predictions_train = np.zeros_like(y_train, dtype=np.float64)\n",
    "predictions_test = np.zeros_like(y_test, dtype=np.float64)\n",
    "\n",
    "# Train separate boosting models for each feature\n",
    "for i in range(n_features):\n",
    "    # Initialize and train the model\n",
    "    model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.001, max_depth=3, random_state=42)\n",
    "    model.fit(X_train[:, i].reshape(-1, 1), y_train)\n",
    "    models.append(model)\n",
    "    \n",
    "    # Summing probability contributions of each feature\n",
    "    predictions_train += model.predict_proba(X_train[:, i].reshape(-1, 1))[:, 1]\n",
    "    predictions_test += model.predict_proba(X_test[:, i].reshape(-1, 1))[:, 1]\n",
    "\n",
    "# Evaluate performance\n",
    "mse = mean_squared_error(y_test, predictions_test)\n",
    "auc = roc_auc_score(y_test, predictions_test)\n",
    "\n",
    "print(f\"EBM-like Model MSE: {mse:.4f}\")\n",
    "print(f\"EBM-like Model AUC: {auc:.4f}\")\n",
    "\n",
    "# Visualize learned feature contributions\n",
    "fig, axes = plt.subplots((n_features // 2) + 1, 2, figsize=(15, 9))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    x_range = np.linspace(X_train[:, i].min(), X_train[:, i].max(), 100).reshape(-1, 1)\n",
    "    y_pred = model.predict_proba(x_range)[:, 1]\n",
    "    axes[i].plot(x_range, y_pred, label=f\"{feature_names[i]}\")\n",
    "    axes[i].set_title(f\"Feature: {feature_names[i]}\")\n",
    "    axes[i].set_xlabel(\"Feature Value\")  # X-axis: Feature value\n",
    "    axes[i].set_ylabel(\"Predicted Probability\")  # Y-axis: Predicted probability of class 1\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()  # Fix layout to prevent overlapping\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from interpret.glassbox import ExplainableBoostingClassifier  # Official EBM model\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "# Select the 8 most important features according to ANOVA F-test\n",
    "selector = SelectKBest(f_classif, k=8)\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Get the names of the selected features (if feature_names is available)\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "selected_feature_names = [feature_names[i] for i in selected_feature_indices]\n",
    "print(\"Selected Features:\", selected_feature_names)\n",
    "\n",
    "# Train separate boosting models for each selected feature (EBM-like model)\n",
    "n_selected_features = X_train_selected.shape[1]\n",
    "models_selected = []\n",
    "predictions_train_selected = np.zeros_like(y_train, dtype=np.float64)\n",
    "predictions_test_selected = np.zeros_like(y_test, dtype=np.float64)\n",
    "\n",
    "for i in range(n_selected_features):\n",
    "    model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.01, max_depth=3, random_state=42)\n",
    "    model.fit(X_train_selected[:, i].reshape(-1, 1), y_train)\n",
    "    models_selected.append(model)\n",
    "    \n",
    "    # Summing probability contributions of each feature\n",
    "    predictions_train_selected += model.predict_proba(X_train_selected[:, i].reshape(-1, 1))[:, 1]\n",
    "    predictions_test_selected += model.predict_proba(X_test_selected[:, i].reshape(-1, 1))[:, 1]\n",
    "\n",
    "# Evaluate performance of the EBM-like model\n",
    "auc_selected = roc_auc_score(y_test, predictions_test_selected)\n",
    "print(f\"EBM-like Model with Selected Features AUC: {auc_selected:.4f}\")\n",
    "\n",
    "# Train benchmark models with selected features\n",
    "# Logistic Regression\n",
    "log_reg_selected = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000, random_state=42))\n",
    "log_reg_selected.fit(X_train_selected, y_train)\n",
    "log_reg_pred_selected = log_reg_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Decision Tree Classifier\n",
    "tree_clf_selected = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "tree_clf_selected.fit(X_train_selected, y_train)\n",
    "tree_clf_pred_selected = tree_clf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "gb_clf_selected = GradientBoostingClassifier(n_estimators=100, learning_rate=0.01, max_depth=3, random_state=42)\n",
    "gb_clf_selected.fit(X_train_selected, y_train)\n",
    "gb_clf_pred_selected = gb_clf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Official EBM Model\n",
    "ebm = ExplainableBoostingClassifier(random_state=42)\n",
    "ebm.fit(X_train_selected, y_train)\n",
    "ebm_pred_selected = ebm.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Evaluate performance of benchmark models\n",
    "log_reg_auc_selected = roc_auc_score(y_test, log_reg_pred_selected)\n",
    "log_reg_recall_selected = recall_score(y_test, log_reg_pred_selected.round())\n",
    "log_reg_balanced_acc_selected = balanced_accuracy_score(y_test, log_reg_pred_selected.round())\n",
    "\n",
    "tree_clf_auc_selected = roc_auc_score(y_test, tree_clf_pred_selected)\n",
    "tree_clf_recall_selected = recall_score(y_test, tree_clf_pred_selected.round())\n",
    "tree_clf_balanced_acc_selected = balanced_accuracy_score(y_test, tree_clf_pred_selected.round())\n",
    "\n",
    "gb_clf_auc_selected = roc_auc_score(y_test, gb_clf_pred_selected)\n",
    "gb_clf_recall_selected = recall_score(y_test, gb_clf_pred_selected.round())\n",
    "gb_clf_balanced_acc_selected = balanced_accuracy_score(y_test, gb_clf_pred_selected.round())\n",
    "\n",
    "ebm_auc_selected = roc_auc_score(y_test, ebm_pred_selected)\n",
    "ebm_recall_selected = recall_score(y_test, ebm_pred_selected.round())\n",
    "ebm_balanced_acc_selected = balanced_accuracy_score(y_test, ebm_pred_selected.round())\n",
    "\n",
    "print(f\"Logistic Regression with Selected Features AUC: {log_reg_auc_selected:.4f}, Recall: {log_reg_recall_selected:.4f}, Balanced Accuracy: {log_reg_balanced_acc_selected:.4f}\")\n",
    "print(f\"Decision Tree Classifier with Selected Features AUC: {tree_clf_auc_selected:.4f}, Recall: {tree_clf_recall_selected:.4f}, Balanced Accuracy: {tree_clf_balanced_acc_selected:.4f}\")\n",
    "print(f\"Gradient Boosting Classifier with Selected Features AUC: {gb_clf_auc_selected:.4f}, Recall: {gb_clf_recall_selected:.4f}, Balanced Accuracy: {gb_clf_balanced_acc_selected:.4f}\")\n",
    "print(f\"Official EBM Model with Selected Features AUC: {ebm_auc_selected:.4f}, Recall: {ebm_recall_selected:.4f}, Balanced Accuracy: {ebm_balanced_acc_selected:.4f}\")\n",
    "tree_clf_auc_selected = roc_auc_score(y_test, tree_clf_pred_selected)\n",
    "gb_clf_auc_selected = roc_auc_score(y_test, gb_clf_pred_selected)\n",
    "ebm_auc_selected = roc_auc_score(y_test, ebm_pred_selected)\n",
    "\n",
    "print(f\"Logistic Regression with Selected Features AUC: {log_reg_auc_selected:.4f}\")\n",
    "print(f\"Decision Tree Classifier with Selected Features AUC: {tree_clf_auc_selected:.4f}\")\n",
    "print(f\"Gradient Boosting Classifier with Selected Features AUC: {gb_clf_auc_selected:.4f}\")\n",
    "print(f\"Official EBM Model with Selected Features AUC: {ebm_auc_selected:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Calculate ROC curves\n",
    "fpr_ebm_selected, tpr_ebm_selected, _ = roc_curve(y_test, predictions_test_selected)\n",
    "fpr_log_reg_selected, tpr_log_reg_selected, _ = roc_curve(y_test, log_reg_pred_selected)\n",
    "fpr_tree_clf_selected, tpr_tree_clf_selected, _ = roc_curve(y_test, tree_clf_pred_selected)\n",
    "fpr_ebm, tpr_ebm, _ = roc_curve(y_test, ebm_pred_selected)\n",
    "fpr_gb_clf_selected, tpr_gb_clf_selected, _ = roc_curve(y_test, gb_clf_pred_selected)\n",
    "\n",
    "# Calculate AUC values\n",
    "auc_ebm_selected = auc(fpr_ebm_selected, tpr_ebm_selected)\n",
    "auc_log_reg_selected = auc(fpr_log_reg_selected, tpr_log_reg_selected)\n",
    "auc_tree_clf_selected = auc(fpr_tree_clf_selected, tpr_tree_clf_selected)\n",
    "auc_ebm = auc(fpr_ebm, tpr_ebm)\n",
    "auc_gb_clf_selected = auc(fpr_gb_clf_selected, tpr_gb_clf_selected)\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr_ebm_selected, tpr_ebm_selected, label=f'EBM-like Model with Selected Features (AUC = {auc_ebm_selected:.4f})')\n",
    "plt.plot(fpr_log_reg_selected, tpr_log_reg_selected, label=f'Logistic Regression with Selected Features (AUC = {auc_log_reg_selected:.4f})')\n",
    "plt.plot(fpr_tree_clf_selected, tpr_tree_clf_selected, label=f'Decision Tree Classifier with Selected Features (AUC = {auc_tree_clf_selected:.4f})')\n",
    "plt.plot(fpr_ebm, tpr_ebm, label=f'Official EBM Model (AUC = {auc_ebm:.4f})')\n",
    "plt.plot(fpr_gb_clf_selected, tpr_gb_clf_selected, label=f'Gradient Boosting Classifier with Selected Features (AUC = {auc_gb_clf_selected:.4f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Models with Selected Features')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def ebm_like_fit(X_train, y_train, X_test, y_test, feature_names, categorical_features=None, n_estimators=100, learning_rate=0.01, max_depth=3):\n",
    "    \"\"\"\n",
    "    Train an EBM-like model with feature interactions, bagging, and early stopping.\n",
    "\n",
    "    Args:\n",
    "        X_train: Training feature matrix (NumPy array).\n",
    "        y_train: Training labels (NumPy array).\n",
    "        X_test: Testing feature matrix (NumPy array).\n",
    "        y_test: Testing labels (NumPy array).\n",
    "        feature_names: List of feature names.\n",
    "        categorical_features: List of indices for categorical features (optional).\n",
    "        n_estimators: Number of estimators for GradientBoostingClassifier.\n",
    "        learning_rate: Learning rate for GradientBoostingClassifier.\n",
    "        max_depth: Maximum depth for GradientBoostingClassifier.\n",
    "\n",
    "    Returns:\n",
    "        predictions_test: Combined predictions for the test set.\n",
    "        models: List of trained models for each feature and interaction.\n",
    "    \"\"\"\n",
    "    # Handle categorical features\n",
    "    if categorical_features is not None:\n",
    "        encoder = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
    "        X_train_cat = encoder.fit_transform(X_train[:, categorical_features])\n",
    "        X_test_cat = encoder.transform(X_test[:, categorical_features])\n",
    "        X_train = np.hstack([X_train[:, ~np.isin(np.arange(X_train.shape[1]), categorical_features)], X_train_cat])\n",
    "        X_test = np.hstack([X_test[:, ~np.isin(np.arange(X_test.shape[1]), categorical_features)], X_test_cat])\n",
    "\n",
    "    n_features = X_train.shape[1]\n",
    "    models = []\n",
    "    predictions_train = np.zeros_like(y_train, dtype=np.float64)\n",
    "    predictions_test = np.zeros_like(y_test, dtype=np.float64)\n",
    "\n",
    "    # Train separate boosting models for each feature\n",
    "    for i in range(n_features):\n",
    "        # Use grid search to find the best hyperparameters\n",
    "        param_grid = {\n",
    "                'n_estimators': 50,\n",
    "                'learning_rate': 0.01,\n",
    "                'max_depth': 3,\n",
    "                'random_state' : 42,\n",
    "            }\n",
    "        model = GradientBoostingClassifier(**param_grid)\n",
    "        model.fit(X_train[:, i].reshape(-1, 1), y_train)\n",
    "        \n",
    "\n",
    "        # Use bagging to reduce variance\n",
    "        bagged_model = BaggingClassifier(\n",
    "            estimator=model,\n",
    "            n_estimators=10,\n",
    "            random_state=42\n",
    "        )\n",
    "        bagged_model.fit(X_train[:, i].reshape(-1, 1), y_train)\n",
    "        models.append(bagged_model)\n",
    "\n",
    "        # Summing probability contributions of each feature\n",
    "        predictions_train += bagged_model.predict_proba(X_train[:, i].reshape(-1, 1))[:, 1]\n",
    "        predictions_test += bagged_model.predict_proba(X_test[:, i].reshape(-1, 1))[:, 1]\n",
    "\n",
    "    # Train models for pairwise interactions\n",
    "    for i in range(n_features):\n",
    "        for j in range(i + 1, n_features):\n",
    "            # Use grid search to find the best hyperparameters\n",
    "            param_grid = {\n",
    "                'n_estimators': 50,\n",
    "                'learning_rate': 0.01,\n",
    "                'max_depth': 3,\n",
    "                'random_state' : 42,\n",
    "            }\n",
    "            model = GradientBoostingClassifier(**param_grid)\n",
    "            model.fit(X_train[:, [i, j]], y_train)\n",
    "            \n",
    "\n",
    "            # Use bagging to reduce variance\n",
    "            bagged_model = BaggingClassifier(\n",
    "                estimator=model,\n",
    "                n_estimators=10,\n",
    "                random_state=42\n",
    "            )\n",
    "            bagged_model.fit(X_train[:, [i, j]], y_train)\n",
    "            models.append(bagged_model)\n",
    "\n",
    "            # Summing probability contributions of each interaction\n",
    "            predictions_train += bagged_model.predict_proba(X_train[:, [i, j]])[:, 1]\n",
    "            predictions_test += bagged_model.predict_proba(X_test[:, [i, j]])[:, 1]\n",
    "\n",
    "    from scipy.special import expit  # Logistic function (sigmoid)\n",
    "\n",
    "    # # Apply logistic transformation\n",
    "    # predictions_train = expit(predictions_train)\n",
    "    # predictions_test = expit(predictions_test)\n",
    "\n",
    "    # Evaluate performance\n",
    "    mse = mean_squared_error(y_test, predictions_test)\n",
    "    auc = roc_auc_score(y_test, predictions_test)\n",
    "\n",
    "    print(f\"EBM-like Model MSE: {mse:.4f}\")\n",
    "    print(f\"EBM-like Model AUC: {auc:.4f}\")\n",
    "\n",
    "    # Visualize learned feature contributions\n",
    "    fig, axes = plt.subplots((n_features // 2) + 1, 2, figsize=(15, 9))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i, model in enumerate(models[:n_features]):  # Only plot individual features\n",
    "        x_range = np.linspace(X_train[:, i].min(), X_train[:, i].max(), 100).reshape(-1, 1)\n",
    "        y_pred = model.predict_proba(x_range)[:, 1]\n",
    "        axes[i].plot(x_range, y_pred, label=f\"{feature_names[i]}\")\n",
    "        axes[i].set_title(f\"Feature: {feature_names[i]}\")\n",
    "        axes[i].set_xlabel(\"Feature Value\")\n",
    "        axes[i].set_ylabel(\"Predicted Probability\")\n",
    "        axes[i].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return predictions_test, models\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test, and feature_names are already defined\n",
    "predictions_test, models = ebm_like_fit(X_train, y_train, X_test, y_test, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, balanced_accuracy_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Function to evaluate classifiers\n",
    "def evaluate_classifiers(X_train, X_test, y_train, y_test):\n",
    "    classifiers = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.01, max_depth=3, random_state=42)\n",
    "    }\n",
    "    \n",
    "    metrics = {\n",
    "        'AUC': roc_auc_score,\n",
    "        'Balanced Accuracy': balanced_accuracy_score,\n",
    "        'Recall': recall_score\n",
    "    }\n",
    "    \n",
    "    results = {name: {} for name in classifiers.keys()}\n",
    "    \n",
    "    for clf_name, clf in classifiers.items():\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        for metric_name, metric_func in metrics.items():\n",
    "            if metric_name == 'AUC':\n",
    "                score = metric_func(y_test, y_pred_proba)\n",
    "            else:\n",
    "                score = metric_func(y_test, y_pred)\n",
    "            results[clf_name][metric_name] = score\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Prepare datasets\n",
    "risk_df_zero_min = risk_df.copy()\n",
    "risk_df_zero_min['risk_score'] = risk_df_zero_min['risk_score'] - risk_df_zero_min['risk_score'].min()\n",
    "\n",
    "# Filter the second dataset to include only samples where risk_score > 500\n",
    "risk_df_500_min = risk_df[risk_df['risk_score'] > 500].copy()\n",
    "\n",
    "# Split datasets\n",
    "X_zero_min = risk_df_zero_min[['risk_score']]\n",
    "y_zero_min = risk_df_zero_min['target']\n",
    "X_train_zero_min, X_test_zero_min, y_train_zero_min, y_test_zero_min = train_test_split(X_zero_min, y_zero_min, test_size=0.2, random_state=42)\n",
    "\n",
    "X_500_min = risk_df_500_min[['risk_score']]\n",
    "y_500_min = risk_df_500_min['target']\n",
    "X_train_500_min, X_test_500_min, y_train_500_min, y_test_500_min = train_test_split(X_500_min, y_500_min, test_size=0.2, random_state=42)\n",
    "\n",
    "# Evaluate classifiers\n",
    "results_zero_min = evaluate_classifiers(X_train_zero_min, X_test_zero_min, y_train_zero_min, y_test_zero_min)\n",
    "results_500_min = evaluate_classifiers(X_train_500_min, X_test_500_min, y_train_500_min, y_test_500_min)\n",
    "\n",
    "print(\"Evaluation Metrics with Risk Score Min = 0:\")\n",
    "for clf_name, metrics in results_zero_min.items():\n",
    "    print(f\"{clf_name}:\")\n",
    "    for metric_name, score in metrics.items():\n",
    "        print(f\"  {metric_name}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluation Metrics with Risk Score > 500:\")\n",
    "for clf_name, metrics in results_500_min.items():\n",
    "    print(f\"{clf_name}:\")\n",
    "    for metric_name, score in metrics.items():\n",
    "        print(f\"  {metric_name}: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "py10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
