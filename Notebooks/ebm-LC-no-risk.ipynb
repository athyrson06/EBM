{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils import Bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dice_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 10:15:18.689818: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-28 10:15:19.124893: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-28 10:15:19.187248: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-03-28 10:15:19.187296: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-03-28 10:15:19.287633: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-28 10:15:21.021743: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-03-28 10:15:21.021851: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-03-28 10:15:21.021859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2025-03-28 10:15:22.050054: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2025-03-28 10:15:22.050275: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-03-28 10:15:22.050308: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (fedora): /proc/driver/nvidia/version does not exist\n",
      "2025-03-28 10:15:22.051064: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import dice_ml\n",
    "from dice_ml.utils import helpers # helper functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = helpers.load_adult_income_dataset()\n",
    "target = dataset[\"income\"] # outcome variable\n",
    "train_dataset, test_dataset, _, _ = train_test_split(dataset,\n",
    "                                                     target,\n",
    "                                                     test_size=0.2,\n",
    "                                                     random_state=0,\n",
    "                                                     stratify=target)\n",
    "# Dataset for training an ML model\n",
    "d = dice_ml.Data(dataframe=train_dataset,\n",
    "                 continuous_features=['age', 'hours_per_week'],\n",
    "                 outcome_name='income')\n",
    "\n",
    "# Pre-trained ML model\n",
    "m = dice_ml.Model(model_path=dice_ml.utils.helpers.get_adult_income_modelpath(),\n",
    "                  backend='TF2', func=\"ohe-min-max\")\n",
    "# DiCE explanation instance\n",
    "exp = dice_ml.Dice(d,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12212/2038679434.py:8: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize):\n",
      "/tmp/ipykernel_12212/2038679434.py:8: DtypeWarning: Columns (19,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize):\n",
      "/tmp/ipykernel_12212/2038679434.py:8: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize):\n",
      "/tmp/ipykernel_12212/2038679434.py:8: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize):\n",
      "/tmp/ipykernel_12212/2038679434.py:8: DtypeWarning: Columns (0,19,59,118) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize):\n",
      "/tmp/ipykernel_12212/2038679434.py:8: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize):\n",
      "/tmp/ipykernel_12212/2038679434.py:8: DtypeWarning: Columns (0,19,118) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize):\n",
      "/tmp/ipykernel_12212/2038679434.py:8: DtypeWarning: Columns (0,19,118,129,130,131,134,135,136,139,145,146,147) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize):\n",
      "/tmp/ipykernel_12212/2038679434.py:8: DtypeWarning: Columns (0,129,130,131,134,135,136,139,145,146,147) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize):\n",
      "/tmp/ipykernel_12212/2038679434.py:8: DtypeWarning: Columns (0,19,118) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize):\n",
      "/tmp/ipykernel_12212/2038679434.py:8: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize):\n",
      "/tmp/ipykernel_12212/2038679434.py:8: DtypeWarning: Columns (0,19,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize):\n",
      "/tmp/ipykernel_12212/2038679434.py:8: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize):\n",
      "/tmp/ipykernel_12212/2038679434.py:8: DtypeWarning: Columns (0,19,59,118,129,130,131,134,135,136,139,145,146,147) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize):\n",
      "/tmp/ipykernel_12212/2038679434.py:8: DtypeWarning: Columns (0,129,130,131,134,135,136,139,145,146,147) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize):\n",
      "/tmp/ipykernel_12212/2038679434.py:8: DtypeWarning: Columns (129,130,131,134,135,136,139) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize):\n",
      "/tmp/ipykernel_12212/2038679434.py:8: DtypeWarning: Columns (0,19,49,59,118,129,130,131,134,135,136,139) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize):\n",
      "/tmp/ipykernel_12212/2038679434.py:8: DtypeWarning: Columns (0,19,49,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize):\n",
      "/tmp/ipykernel_12212/2038679434.py:8: DtypeWarning: Columns (129,130,131,134,135,136,139) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize):\n",
      "/tmp/ipykernel_12212/2038679434.py:8: DtypeWarning: Columns (0,19,49,59,129,130,131,134,135,136,139) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize):\n",
      "/tmp/ipykernel_12212/2038679434.py:8: DtypeWarning: Columns (0,19,59,118) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize):\n",
      "/tmp/ipykernel_12212/2038679434.py:8: DtypeWarning: Columns (0,118) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize):\n",
      "/tmp/ipykernel_12212/2038679434.py:8: DtypeWarning: Columns (0,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize):\n"
     ]
    }
   ],
   "source": [
    "# Define the chunk size for reading the CSV file\n",
    "chunksize = 100000  # Adjust this value based on your requirements\n",
    "\n",
    "# Initialize an empty list to store filtered chunks\n",
    "filtered_chunks = []\n",
    "\n",
    "# Read the CSV file in chunks based on the defined chunk size\n",
    "for chunk in pd.read_csv(\"/home/athyrson/Code/Data/Raw Data/Lending Club/accepted_2007_to_2018Q4.csv\", chunksize=chunksize):\n",
    "    # Filter the current chunk based on the criteria\n",
    "    filtered_chunk = chunk[chunk['issue_d'].str.contains(\"2010\", na=False)]\n",
    "    # filtered_chunk = filtered_chunk[~filtered_chunk['issue_d'].str.contains(\"Oct-2013|Nov-2013|Dec-2013\", na=False)]\n",
    "    # Append the filtered chunk to the list\n",
    "    filtered_chunks.append(filtered_chunk)\n",
    "\n",
    "# Concatenate all filtered chunks into a single DataFrame\n",
    "df = pd.concat(filtered_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12537, 151)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = df.sample(frac=1, random_state=42)\n",
    "df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Calculate risk score\n",
    "    df[\"risk_score\"] = df.loc[:, [\"last_fico_range_high\", \"last_fico_range_low\"]].mean(axis=1)\n",
    "    \n",
    "    # Create target variable\n",
    "    df[\"target\"] = np.where((df.loan_status == 'Current') |\n",
    "                            (df.loan_status == 'Fully Paid') |\n",
    "                            (df.loan_status == \"Issued\") |\n",
    "                            (df.loan_status == 'Does not meet the credit policy. Status:Fully Paid'), 0, 1)\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    for c in ['last_fico_range_high', 'last_fico_range_low', 'loan_status']:\n",
    "        df = df.drop(c, axis=1, errors='ignore')\n",
    "    \n",
    "    def fix_term_column(df):\n",
    "        \"\"\"\n",
    "        Fix the dtype of the 'term' column by extracting the numeric part and converting it to an integer.\n",
    "        Handles cases where the column contains non-string values (e.g., NaN or float).\n",
    "        \n",
    "        Parameters:\n",
    "            df (pd.DataFrame): The DataFrame containing the 'term' column.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame with the 'term' column converted to numeric.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract numeric part from the 'term' column and convert to integer\n",
    "            df['term'] = df['term'].apply(lambda x: int(re.search(r'\\d+', str(x)).group()) if pd.notna(x) else x)\n",
    "        except AttributeError as e:\n",
    "            # Handle cases where the regex search fails (e.g., no match found)\n",
    "            print(\"Error: Unable to extract numeric value from 'term' column.\")\n",
    "            print(f\"Details: {e}\")\n",
    "        except Exception as e:\n",
    "            # Handle any other unexpected errors\n",
    "            print(\"An unexpected error occurred while fixing the 'term' column.\")\n",
    "            print(f\"Details: {e}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def fix_emp_length_column(df):\n",
    "        \"\"\"\n",
    "        Fix the dtype of the 'emp_length' column by converting it to numeric.\n",
    "        Handles cases where the column contains non-string values (e.g., NaN or float).\n",
    "        \n",
    "        Parameters:\n",
    "            df (pd.DataFrame): The DataFrame containing the 'emp_length' column.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame with the 'emp_length' column converted to numeric.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Map '< 1 year' to '0' and extract numeric part from the 'emp_length' column\n",
    "            df['emp_length'] = df['emp_length'].map(lambda x: \"0\" if x == '< 1 year' else x)\n",
    "            df['emp_length'] = df['emp_length'].map(lambda x: int(re.search(r'\\d+', str(x)).group()), na_action='ignore')\n",
    "        except Exception as e:\n",
    "            # Handle any unexpected errors\n",
    "            print(\"Error: Unable to convert 'emp_length' column to numeric.\")\n",
    "            print(f\"Details: {e}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    df = fix_term_column(df)\n",
    "    df = fix_emp_length_column(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>...</th>\n",
       "      <th>disbursement_method</th>\n",
       "      <th>debt_settlement_flag</th>\n",
       "      <th>debt_settlement_flag_date</th>\n",
       "      <th>settlement_status</th>\n",
       "      <th>settlement_date</th>\n",
       "      <th>settlement_amount</th>\n",
       "      <th>settlement_percentage</th>\n",
       "      <th>settlement_term</th>\n",
       "      <th>risk_score</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1643125</th>\n",
       "      <td>498726</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>36</td>\n",
       "      <td>11.36</td>\n",
       "      <td>164.56</td>\n",
       "      <td>B</td>\n",
       "      <td>B5</td>\n",
       "      <td>...</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>732.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636584</th>\n",
       "      <td>597920</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9861.357548</td>\n",
       "      <td>36</td>\n",
       "      <td>7.88</td>\n",
       "      <td>312.82</td>\n",
       "      <td>A</td>\n",
       "      <td>A5</td>\n",
       "      <td>...</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>682.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1639847</th>\n",
       "      <td>543951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8700.0</td>\n",
       "      <td>8700.0</td>\n",
       "      <td>8700.000000</td>\n",
       "      <td>36</td>\n",
       "      <td>7.51</td>\n",
       "      <td>270.67</td>\n",
       "      <td>A</td>\n",
       "      <td>A4</td>\n",
       "      <td>...</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>787.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643531</th>\n",
       "      <td>492732</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>36</td>\n",
       "      <td>7.88</td>\n",
       "      <td>125.13</td>\n",
       "      <td>A</td>\n",
       "      <td>A5</td>\n",
       "      <td>...</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>712.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640954</th>\n",
       "      <td>528880</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9362.071323</td>\n",
       "      <td>36</td>\n",
       "      <td>7.14</td>\n",
       "      <td>309.42</td>\n",
       "      <td>A</td>\n",
       "      <td>A3</td>\n",
       "      <td>...</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>822.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652094</th>\n",
       "      <td>562187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13800.0</td>\n",
       "      <td>13800.0</td>\n",
       "      <td>13800.000000</td>\n",
       "      <td>36</td>\n",
       "      <td>16.32</td>\n",
       "      <td>487.36</td>\n",
       "      <td>D</td>\n",
       "      <td>D5</td>\n",
       "      <td>...</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>602.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638791</th>\n",
       "      <td>562457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.000000</td>\n",
       "      <td>36</td>\n",
       "      <td>7.14</td>\n",
       "      <td>247.53</td>\n",
       "      <td>A</td>\n",
       "      <td>A3</td>\n",
       "      <td>...</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>812.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638990</th>\n",
       "      <td>557251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>4500.000000</td>\n",
       "      <td>36</td>\n",
       "      <td>14.84</td>\n",
       "      <td>172.94</td>\n",
       "      <td>D</td>\n",
       "      <td>D1</td>\n",
       "      <td>...</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>707.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634460</th>\n",
       "      <td>624384</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9950.000000</td>\n",
       "      <td>60</td>\n",
       "      <td>13.35</td>\n",
       "      <td>229.33</td>\n",
       "      <td>C</td>\n",
       "      <td>C4</td>\n",
       "      <td>...</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>702.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640870</th>\n",
       "      <td>530129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>36</td>\n",
       "      <td>15.58</td>\n",
       "      <td>69.91</td>\n",
       "      <td>D</td>\n",
       "      <td>D3</td>\n",
       "      <td>...</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>662.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12537 rows Ã— 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  member_id  loan_amnt  funded_amnt  funded_amnt_inv  term  \\\n",
       "1643125  498726        NaN     5000.0       5000.0      5000.000000    36   \n",
       "1636584  597920        NaN    10000.0      10000.0      9861.357548    36   \n",
       "1639847  543951        NaN     8700.0       8700.0      8700.000000    36   \n",
       "1643531  492732        NaN     4000.0       4000.0      4000.000000    36   \n",
       "1640954  528880        NaN    10000.0      10000.0      9362.071323    36   \n",
       "...         ...        ...        ...          ...              ...   ...   \n",
       "1652094  562187        NaN    13800.0      13800.0     13800.000000    36   \n",
       "1638791  562457        NaN     8000.0       8000.0      8000.000000    36   \n",
       "1638990  557251        NaN     5000.0       5000.0      4500.000000    36   \n",
       "1634460  624384        NaN    10000.0      10000.0      9950.000000    60   \n",
       "1640870  530129        NaN     2000.0       2000.0      2000.000000    36   \n",
       "\n",
       "         int_rate  installment grade sub_grade  ... disbursement_method  \\\n",
       "1643125     11.36       164.56     B        B5  ...                Cash   \n",
       "1636584      7.88       312.82     A        A5  ...                Cash   \n",
       "1639847      7.51       270.67     A        A4  ...                Cash   \n",
       "1643531      7.88       125.13     A        A5  ...                Cash   \n",
       "1640954      7.14       309.42     A        A3  ...                Cash   \n",
       "...           ...          ...   ...       ...  ...                 ...   \n",
       "1652094     16.32       487.36     D        D5  ...                Cash   \n",
       "1638791      7.14       247.53     A        A3  ...                Cash   \n",
       "1638990     14.84       172.94     D        D1  ...                Cash   \n",
       "1634460     13.35       229.33     C        C4  ...                Cash   \n",
       "1640870     15.58        69.91     D        D3  ...                Cash   \n",
       "\n",
       "         debt_settlement_flag debt_settlement_flag_date  settlement_status  \\\n",
       "1643125                     N                       NaN                NaN   \n",
       "1636584                     N                       NaN                NaN   \n",
       "1639847                     N                       NaN                NaN   \n",
       "1643531                     N                       NaN                NaN   \n",
       "1640954                     N                       NaN                NaN   \n",
       "...                       ...                       ...                ...   \n",
       "1652094                     N                       NaN                NaN   \n",
       "1638791                     N                       NaN                NaN   \n",
       "1638990                     N                       NaN                NaN   \n",
       "1634460                     N                       NaN                NaN   \n",
       "1640870                     N                       NaN                NaN   \n",
       "\n",
       "        settlement_date settlement_amount settlement_percentage  \\\n",
       "1643125             NaN               NaN                   NaN   \n",
       "1636584             NaN               NaN                   NaN   \n",
       "1639847             NaN               NaN                   NaN   \n",
       "1643531             NaN               NaN                   NaN   \n",
       "1640954             NaN               NaN                   NaN   \n",
       "...                 ...               ...                   ...   \n",
       "1652094             NaN               NaN                   NaN   \n",
       "1638791             NaN               NaN                   NaN   \n",
       "1638990             NaN               NaN                   NaN   \n",
       "1634460             NaN               NaN                   NaN   \n",
       "1640870             NaN               NaN                   NaN   \n",
       "\n",
       "        settlement_term risk_score target  \n",
       "1643125             NaN      732.0      0  \n",
       "1636584             NaN      682.0      0  \n",
       "1639847             NaN      787.0      0  \n",
       "1643531             NaN      712.0      0  \n",
       "1640954             NaN      822.0      0  \n",
       "...                 ...        ...    ...  \n",
       "1652094             NaN      602.0      0  \n",
       "1638791             NaN      812.0      0  \n",
       "1638990             NaN      707.0      0  \n",
       "1634460             NaN      702.0      0  \n",
       "1640870             NaN      662.0      0  \n",
       "\n",
       "[12537 rows x 150 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = preprocess_data(df_1)\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bunch_df(X,y):\n",
    "    # Convert to NumPy arrays\n",
    "    X_array = X.to_numpy()\n",
    "    y_array = y.to_numpy()\n",
    "\n",
    "    # Get feature names\n",
    "    feature_names = X.columns.tolist()\n",
    "\n",
    "    # Create a Bunch object\n",
    "    bunch_df = Bunch(\n",
    "        data=X_array,\n",
    "        target=y_array,\n",
    "        feature_names=feature_names,\n",
    "        DESCR=\"Custom dataset similar to scikit-learn fetch_datasets\"\n",
    "    )\n",
    "    return bunch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = [\n",
    "    'issue_d', 'url', 'collection_recovery_fee', 'last_pymnt_amnt', 'last_pymnt_d', 'pymnt_plan', 'recoveries', \n",
    "    'total_pymnt', 'total_pymnt_inv', 'total_rec_int', 'total_rec_late_fee', 'total_rec_prncp', 'hardship_flag', \n",
    "    'hardship_type', 'hardship_reason', 'hardship_status', 'deferral_term', 'hardship_amount', 'hardship_start_date', \n",
    "    'hardship_end_date', 'payment_plan_start_date', 'hardship_length', 'hardship_dpd', 'hardship_loan_status', \n",
    "    'orig_projected_additional_accrued_interest', 'hardship_payoff_balance_amount', 'hardship_last_payment_amount', \n",
    "    'debt_settlement_flag', 'debt_settlement_flag_date', 'settlement_status', 'settlement_date', 'settlement_amount', \n",
    "    'settlement_percentage', 'settlement_term', 'id', 'emp_title', 'desc', 'title', 'last_credit_pull_d', 'next_pymnt_d', 'risk_score'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_2.loc[:, df_2.columns != 'target'].drop(drop_list, axis=1)#, 'risk_score'\n",
    "y = df_2['target']\n",
    "\n",
    "df_3 = create_bunch_df(X, y)\n",
    "X = df_3.data\n",
    "y = df_3.target\n",
    "feature_names = df_3.feature_names\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Explainable Boosting Machine (EBM)</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Model    AUC  Recall  Balanced Accuracy\n",
       "0                 Logistic Regression  0.678   0.023              0.508\n",
       "1            Decision Tree Classifier  0.642   0.017              0.504\n",
       "2        Gradient Boosting Classifier  0.663   0.000              0.500\n",
       "3  Explainable Boosting Machine (EBM)  0.678   0.032              0.511"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, recall_score, balanced_accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from interpret.glassbox import ExplainableBoostingClassifier  # Import EBM\n",
    "\n",
    "# Train benchmark models with all features\n",
    "# Logistic Regression\n",
    "log_reg = make_pipeline(TargetEncoder(), StandardScaler(), LogisticRegression(max_iter=1000, random_state=42))\n",
    "log_reg.fit(X_train, y_train)\n",
    "log_reg_pred = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Decision Tree Classifier\n",
    "tree_clf = make_pipeline(TargetEncoder(), StandardScaler(), DecisionTreeClassifier(max_depth=5, random_state=42))\n",
    "tree_clf.fit(X_train, y_train)\n",
    "tree_clf_pred = tree_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "gb_clf = make_pipeline(TargetEncoder(), StandardScaler(), GradientBoostingClassifier(n_estimators=100, learning_rate=0.01, max_depth=3, random_state=42))\n",
    "gb_clf.fit(X_train, y_train)\n",
    "gb_clf_pred = gb_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Explainable Boosting Machine (EBM)\n",
    "ebm = make_pipeline( TargetEncoder(), StandardScaler(), ExplainableBoostingClassifier(random_state=42))\n",
    "ebm.fit(X_train, y_train)\n",
    "ebm_pred = ebm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate performance of benchmark models\n",
    "log_reg_auc = roc_auc_score(y_test, log_reg_pred)\n",
    "log_reg_recall = recall_score(y_test, log_reg_pred.round())\n",
    "log_reg_balanced_acc = balanced_accuracy_score(y_test, log_reg_pred.round())\n",
    "\n",
    "tree_clf_auc = roc_auc_score(y_test, tree_clf_pred)\n",
    "tree_clf_recall = recall_score(y_test, tree_clf_pred.round())\n",
    "tree_clf_balanced_acc = balanced_accuracy_score(y_test, tree_clf_pred.round())\n",
    "\n",
    "gb_clf_auc = roc_auc_score(y_test, gb_clf_pred)\n",
    "gb_clf_recall = recall_score(y_test, gb_clf_pred.round())\n",
    "gb_clf_balanced_acc = balanced_accuracy_score(y_test, gb_clf_pred.round())\n",
    "\n",
    "ebm_auc = roc_auc_score(y_test, ebm_pred)\n",
    "ebm_recall = recall_score(y_test, ebm_pred.round())\n",
    "ebm_balanced_acc = balanced_accuracy_score(y_test, ebm_pred.round())\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame({\n",
    "    \"Model\": [\"Logistic Regression\", \"Decision Tree Classifier\", \"Gradient Boosting Classifier\", \"Explainable Boosting Machine (EBM)\"],\n",
    "    \"AUC\": [log_reg_auc, tree_clf_auc, gb_clf_auc, ebm_auc],\n",
    "    \"Recall\": [log_reg_recall, tree_clf_recall, gb_clf_recall, ebm_recall],\n",
    "    \"Balanced Accuracy\": [log_reg_balanced_acc, tree_clf_balanced_acc, gb_clf_balanced_acc, ebm_balanced_acc]\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "results_df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_2.loc[:, df_2.columns != 'target'].drop(drop_list, axis=1)#, 'risk_score'\n",
    "y = df_2['target']\n",
    "\n",
    "# df_3 = create_bunch_df(X, y)\n",
    "# X = df_3.data\n",
    "# y = df_3.target\n",
    "# feature_names = df_3.feature_names\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/athyrson/Code/py10/lib64/python3.10/site-packages/interpret/utils/_preprocessor.py:358: RuntimeWarning: All-NaN slice encountered\n",
      "  min_feature_val = np.nanmin(X_col)\n",
      "/home/athyrson/Code/py10/lib64/python3.10/site-packages/interpret/utils/_preprocessor.py:359: RuntimeWarning: All-NaN slice encountered\n",
      "  max_feature_val = np.nanmax(X_col)\n",
      "/home/athyrson/Code/py10/lib64/python3.10/site-packages/interpret/glassbox/_ebm/_ebm.py:813: UserWarning: Missing values detected. Our visualizations do not currently display missing values. To retain the glassbox nature of the model you need to either set the missing values to an extreme value like -1000 that will be visible on the graphs, or manually examine the missing value score in ebm.term_scores_[term_index][0]\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Explainable Boosting Machine (EBM)</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Model    AUC  Recall  Balanced Accuracy\n",
       "0                 Logistic Regression  0.678   0.023              0.508\n",
       "1            Decision Tree Classifier  0.642   0.017              0.504\n",
       "2        Gradient Boosting Classifier  0.663   0.000              0.500\n",
       "3  Explainable Boosting Machine (EBM)  0.697   0.026              0.510"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, recall_score, balanced_accuracy_score\n",
    "import pandas as pd\n",
    "from interpret.glassbox import ExplainableBoostingClassifier  # Import EBM\n",
    "\n",
    "# Train benchmark models with all features\n",
    "\n",
    "\n",
    "\n",
    "# Explainable Boosting Machine (EBM)\n",
    "ebm = ExplainableBoostingClassifier(random_state=42)\n",
    "ebm.fit(X_train, y_train)\n",
    "ebm_pred = ebm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate performance of benchmark models\n",
    "ebm_auc = roc_auc_score(y_test, ebm_pred)\n",
    "ebm_recall = recall_score(y_test, ebm_pred.round())\n",
    "ebm_balanced_acc = balanced_accuracy_score(y_test, ebm_pred.round())\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame({\n",
    "    \"Model\": [\"Logistic Regression\", \"Decision Tree Classifier\", \"Gradient Boosting Classifier\", \"Explainable Boosting Machine (EBM)\"],\n",
    "    \"AUC\": [log_reg_auc, tree_clf_auc, gb_clf_auc, ebm_auc],\n",
    "    \"Recall\": [log_reg_recall, tree_clf_recall, gb_clf_recall, ebm_recall],\n",
    "    \"Balanced Accuracy\": [log_reg_balanced_acc, tree_clf_balanced_acc, gb_clf_balanced_acc, ebm_balanced_acc]\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "results_df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7001/139708102980080/ -->\n",
       "<iframe src=\"http://127.0.0.1:7001/139708102980080/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from interpret import show\n",
    "show(ebm.explain_global())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
